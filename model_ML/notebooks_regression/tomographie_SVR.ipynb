{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1c6eb9",
   "metadata": {},
   "source": [
    "# Quantum Tomography: Machine Learning (SVR) vs Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "## Project Overview\n",
    "The goal of this notebook is to benchmark Machine Learning algorithms—specifically **Support Vector Regression (SVR)**—against standard **Maximum Likelihood Estimation (MLE)** for the task of single-qubit quantum tomography.\n",
    "\n",
    "## Problem Statement\n",
    "Quantum tomography consists of reconstructing the quantum state (density matrix $\\rho$) from a set of measurements. \n",
    "- **MLE** is the standard statistical approach but requires computationally expensive optimization for every new state.\n",
    "- **ML** approaches propose to \"learn\" the inversion map offline. Once trained, the reconstruction is nearly instantaneous.\n",
    "\n",
    "## Methodology & Parameters\n",
    "We will explore the performance trade-offs along three dimensions:\n",
    "1.  **Dataset Properties:** Variations in dataset size (`n_states`), measurement noise (`n_shots`), and physical decoherence (`decoherence_level`).\n",
    "2.  **Algorithm (SVR):** Testing different Kernels (RBF, Polynomial, Sigmoid) via `MultiOutputRegressor`.\n",
    "3.  **Hyperparameter Optimization:** Using Grid Search to find optimal $C$, $\\gamma$, and $\\epsilon$ parameters.\n",
    "\n",
    "## Comparison Metrics\n",
    "1.  **Fidelity:** Physical closeness between the reconstructed state and the real state (1.0 is perfect).\n",
    "2.  **MSE (Mean Squared Error):** Euclidean distance in the Bloch sphere.\n",
    "3.  **Computational Cost:** Wall-clock time for reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b19600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Attempt to import the provided local module\n",
    "# Assuming the file 'saint_dtSet.py' is in a folder named 'gemi' or in the root path.\n",
    "try:\n",
    "    from dataset_build.saint_dtSet import generate_qubit_tomography_dataset_base, perform_mle_tomography\n",
    "except ImportError:\n",
    "    # Fallback if the file is in the same directory\n",
    "    try:\n",
    "        from saint_dtSet import generate_qubit_tomography_dataset_base, perform_mle_tomography\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please ensure 'saint_dtSet.py' is in the python path.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# METRICS DEFINITION\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def bloch_to_density_matrix(x, y, z):\n",
    "    \"\"\"\n",
    "    Converts Bloch vector components to a 2x2 density matrix.\n",
    "    rho = 1/2 * (I + x*sigma_x + y*sigma_y + z*sigma_z)\n",
    "    \"\"\"\n",
    "    sigma_x = np.array([[0, 1], [1, 0]], dtype=complex)\n",
    "    sigma_y = np.array([[0, -1j], [1j, 0]], dtype=complex)\n",
    "    sigma_z = np.array([[1, 0], [0, -1]], dtype=complex)\n",
    "    identity = np.eye(2, dtype=complex)\n",
    "    \n",
    "    rho = 0.5 * (identity + x*sigma_x + y*sigma_y + z*sigma_z)\n",
    "    return rho\n",
    "\n",
    "def calculate_fidelity(row, pred_cols=('X_pred', 'Y_pred', 'Z_pred'), real_cols=('X_real', 'Y_real', 'Z_real')):\n",
    "    \"\"\"\n",
    "    Calculates the quantum fidelity between the Real state and the Predicted state.\n",
    "    \n",
    "    Formula for single qubit using Bloch vectors r_real and r_pred:\n",
    "    F(rho, sigma) = 0.5 * (1 + r_real.r_pred + sqrt((1 - |r_real|^2)(1 - |r_pred|^2)))\n",
    "    \n",
    "    Note: This analytic formula is faster than matrix sqrt operations.\n",
    "    \"\"\"\n",
    "    # Extract vectors\n",
    "    r_real = np.array([row[c] for c in real_cols])\n",
    "    r_pred = np.array([row[c] for c in pred_cols])\n",
    "    \n",
    "    # Norms squared\n",
    "    norm2_real = np.dot(r_real, r_real)\n",
    "    norm2_pred = np.dot(r_pred, r_pred)\n",
    "    \n",
    "    # Dot product\n",
    "    dot_prod = np.dot(r_real, r_pred)\n",
    "    \n",
    "    # Safety clip for sqrt (numerical stability)\n",
    "    val_real = max(0.0, 1.0 - norm2_real)\n",
    "    val_pred = max(0.0, 1.0 - norm2_pred)\n",
    "    \n",
    "    fidelity = 0.5 * (1.0 + dot_prod + np.sqrt(val_real * val_pred))\n",
    "    \n",
    "    # Clip fidelity to [0, 1] range to handle float discrepancies\n",
    "    return min(max(fidelity, 0.0), 1.0)\n",
    "\n",
    "def evaluate_performance(df_results, title=\"Evaluation\"):\n",
    "    \"\"\"\n",
    "    Computes global metrics (Average Fidelity, MSE) and prints a summary.\n",
    "    Expects columns: X_real, Y_real, Z_real AND X_pred, Y_pred, Z_pred.\n",
    "    \"\"\"\n",
    "    print(f\"--- {title} ---\")\n",
    "    \n",
    "    # 1. MSE (Mean Squared Error) on Bloch components\n",
    "    y_true = df_results[['X_real', 'Y_real', 'Z_real']].values\n",
    "    y_pred = df_results[['X_pred', 'Y_pred', 'Z_pred']].values\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    # 2. Fidelity\n",
    "    # Apply row-wise calculation\n",
    "    fidelities = df_results.apply(calculate_fidelity, axis=1)\n",
    "    avg_fidelity = fidelities.mean()\n",
    "    std_fidelity = fidelities.std()\n",
    "    \n",
    "    print(f\"MSE (Bloch Vector): {mse:.6f}\")\n",
    "    print(f\"Average Fidelity:   {avg_fidelity:.6f} (+/- {std_fidelity:.6f})\")\n",
    "    \n",
    "    return mse, avg_fidelity\n",
    "\n",
    "print(\"Libraries loaded and Metrics defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffc3397",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd6697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
