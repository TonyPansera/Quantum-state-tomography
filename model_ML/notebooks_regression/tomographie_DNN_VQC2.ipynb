{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 2 \u2014 DNN vs VQC vs baseline\n",
        "Boucle d'exploration pour comparer le DNN, le VQC et une baseline rapide (SVR/MLE approxim\u00e9e) en fonction du niveau de d\u00e9coh\u00e9rence. Les classes `TomographyDNN`, `TomographyVQC` et `QuantumFidelityLoss` sont suppos\u00e9es d\u00e9j\u00e0 d\u00e9finies dans la cellule pr\u00e9c\u00e9dente.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import du g\u00e9n\u00e9rateur de dataset\n",
        "try:\n",
        "    from saint_dtSet import generate_qubit_tomography_dataset_base\n",
        "except ImportError:\n",
        "    from dataset_build.saint_dtSet import generate_qubit_tomography_dataset_base\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "print(f\"Device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def fidelity_from_bloch(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calcul analytique de la fid\u00e9lit\u00e9 entre deux vecteurs de Bloch (qubit).\n",
        "    pred et target: tensors (..., 3).\n",
        "    \"\"\"\n",
        "    dot = (pred * target).sum(dim=-1)\n",
        "    norm_pred = (pred ** 2).sum(dim=-1)\n",
        "    norm_target = (target ** 2).sum(dim=-1)\n",
        "    under_sqrt = torch.clamp(1.0 - norm_pred, min=0.0) * torch.clamp(1.0 - norm_target, min=0.0)\n",
        "    fidelity = 0.5 * (1.0 + dot + torch.sqrt(under_sqrt))\n",
        "    return torch.clamp(fidelity, 0.0, 1.0)\n",
        "\n",
        "\n",
        "def build_dataloaders(df: pd.DataFrame, batch_size: int = 32):\n",
        "    features = df[['X_mean', 'Y_mean', 'Z_mean']].to_numpy(dtype=np.float32)\n",
        "    targets = df[['X_real', 'Y_real', 'Z_real']].to_numpy(dtype=np.float32)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "    train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "    test_ds = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader, (X_test, y_test)\n",
        "\n",
        "\n",
        "def evaluate_model(model, data_loader, device=DEVICE):\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in data_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            pred = model(xb)\n",
        "            preds.append(pred.detach().cpu())\n",
        "            targets.append(yb.detach().cpu())\n",
        "    preds_t = torch.cat(preds, dim=0)\n",
        "    targets_t = torch.cat(targets, dim=0)\n",
        "    fidelities = fidelity_from_bloch(preds_t, targets_t)\n",
        "    return fidelities.mean().item()\n",
        "\n",
        "\n",
        "def compute_baseline_fidelity(x_test: np.ndarray, y_test: np.ndarray) -> float:\n",
        "    # Baseline rapide ~SVR/MLE : pr\u00e9dicteur trivial X_mean->X_real (sans contrainte de sph\u00e8re).\n",
        "    preds = torch.from_numpy(x_test)\n",
        "    targets = torch.from_numpy(y_test)\n",
        "    return fidelity_from_bloch(preds, targets).mean().item()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, optimizer, loss_fn, epochs: int, device=DEVICE):\n",
        "    \"\"\"\n",
        "    Boucle g\u00e9n\u00e9rique d'entra\u00eenement.\n",
        "    D\u00e9place inputs/targets sur DEVICE avant forward comme demand\u00e9.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    history = []\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = loss_fn(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        history.append(epoch_loss)\n",
        "        if epoch == 1 or epoch % max(1, epochs // 5) == 0:\n",
        "            print(f\"Epoch {epoch:>3}/{epochs} - loss: {epoch_loss:.4f}\")\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Boucle d'exp\u00e9rimentation principale\n",
        "# ---------------------------------------------------------------------------\n",
        "N_STATES = 2000\n",
        "N_SHOTS = 500\n",
        "DECOHERENCE_LEVELS = [0.0, 0.2, 0.5, 0.8]\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_DNN = 25\n",
        "EPOCHS_VQC = 12  # VQC plus lent -> moins d'epochs\n",
        "LR_DNN = 0.01\n",
        "LR_VQC = 0.05\n",
        "\n",
        "results = {\"dnn\": [], \"vqc\": [], \"baseline\": []}\n",
        "\n",
        "for level in DECOHERENCE_LEVELS:\n",
        "    print(f\"\n=== D\u00e9coh\u00e9rence {level} ===\")\n",
        "    df = generate_qubit_tomography_dataset_base(\n",
        "        n_states=N_STATES,\n",
        "        n_shots=N_SHOTS,\n",
        "        include_decoherence=True,\n",
        "        decoherence_level=level,\n",
        "        mode=\"finite_shots\",\n",
        "        random_state=1234\n",
        "    )\n",
        "\n",
        "    train_loader, test_loader, (x_test, y_test) = build_dataloaders(df, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # ---------------------- DNN ----------------------\n",
        "    dnn = TomographyDNN().to(DEVICE)\n",
        "    dnn_optimizer = torch.optim.Adam(dnn.parameters(), lr=LR_DNN)\n",
        "    dnn_loss = QuantumFidelityLoss()\n",
        "    _ = train_model(dnn, train_loader, dnn_optimizer, dnn_loss, epochs=EPOCHS_DNN, device=DEVICE)\n",
        "    dnn_fid = evaluate_model(dnn, test_loader, device=DEVICE)\n",
        "\n",
        "    # ---------------------- VQC ----------------------\n",
        "    vqc = TomographyVQC().to(DEVICE)\n",
        "    vqc_optimizer = torch.optim.Adam(vqc.parameters(), lr=LR_VQC)\n",
        "    vqc_loss = QuantumFidelityLoss()\n",
        "    _ = train_model(vqc, train_loader, vqc_optimizer, vqc_loss, epochs=EPOCHS_VQC, device=DEVICE)\n",
        "    vqc_fid = evaluate_model(vqc, test_loader, device=DEVICE)\n",
        "\n",
        "    # -------------------- Baseline --------------------\n",
        "    baseline_fid = compute_baseline_fidelity(x_test, y_test)\n",
        "\n",
        "    results[\"dnn\"].append(dnn_fid)\n",
        "    results[\"vqc\"].append(vqc_fid)\n",
        "    results[\"baseline\"].append(baseline_fid)\n",
        "\n",
        "    print(f\"DNN fidelity (test): {dnn_fid:.4f}\")\n",
        "    print(f\"VQC fidelity (test): {vqc_fid:.4f}\")\n",
        "    print(f\"Baseline fidelity    : {baseline_fid:.4f}\")\n",
        "\n",
        "print('Boucle termin\u00e9e.')\n",
        "results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Visualisation des fid\u00e9lit\u00e9s moyennes\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(DECOHERENCE_LEVELS, results['dnn'], '-o', label='DNN')\n",
        "plt.plot(DECOHERENCE_LEVELS, results['vqc'], '-o', label='VQC')\n",
        "plt.plot(DECOHERENCE_LEVELS, results['baseline'], '-o', label='Baseline (SVR/MLE rapide)')\n",
        "plt.xlabel('Niveau de d\u00e9coh\u00e9rence')\n",
        "plt.ylabel('Fid\u00e9lit\u00e9 moyenne (test)')\n",
        "plt.ylim(0.0, 1.05)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}