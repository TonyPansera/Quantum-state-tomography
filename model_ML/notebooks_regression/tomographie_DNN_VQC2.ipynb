{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 2: Advanced Quantum Tomography\n",
        "## Deep Learning (DNN) & Variational Quantum Circuits (VQC) vs. Classical SVR\n",
        "\n",
        "## 1. Scientific Context & Exploration Goals\n",
        "In the previous notebook, we established a robust baseline using **Support Vector Regression (SVR)**, a classical kernel-based method. While effective, SVR relies on fixed kernels (RBF, Polynomial) which may not perfectly capture the complex geometry of quantum states under decoherence.\n",
        "\n",
        "In this second exploratory phase, we move towards **Advanced Architectures**. Our goal is to determine if models with higher representational capacity (Deep Learning) or native quantum priors (VQC) can surpass the classical baseline, particularly in the regime of **impure states (decoherence)** and **limited data**.\n",
        "\n",
        "We investigate two challengers:\n",
        "1.  **Deep Neural Networks (DNN):** Using the universal approximation theorem to model the mapping from measurements to density matrices with high non-linearity.\n",
        "2.  **Variational Quantum Circuits (VQC):** A \"Quantum Machine Learning\" approach. We hypothesize that a quantum circuit possesses a natural *inductive bias* for quantum data, potentially requiring fewer parameters to represent the Hilbert space than a classical network.\n",
        "\n",
        "## 2. A Paradigm Shift: \"Physics-Informed\" Training\n",
        "Unlike standard regression which minimizes the Euclidean distance (MSE), we introduce a **Custom Loss Function** grounded in Quantum Information Theory.\n",
        "\n",
        "### The \"Fidelity-Based\" Backpropagation\n",
        "Standard ML optimizes geometry. We want to optimize **physics**.\n",
        "Instead of blindingly minimizing $MSE = ||\\vec{r}_{pred} - \\vec{r}_{real}||^2$, we configure our Neural Networks (both Classical and Quantum) to directly maximize the **Quantum Fidelity** ($F$).\n",
        "\n",
        "During the **Backpropagation** pass, the gradient of the Fidelity is computed with respect to the model weights. This forces the optimizer to prioritize directions that increase the physical overlap between the predicted and true states.\n",
        "\n",
        "**The Mathematical Loss Function:**\n",
        "For a single qubit state defined by a Bloch vector $\\vec{r}$, the loss $\\mathcal{L}$ to minimize is:\n",
        "\n",
        "$$\\mathcal{L} = 1 - F(\\rho_{pred}, \\rho_{real})$$\n",
        "\n",
        "Where the Fidelity $F$ for single-qubit Bloch vectors is given analytically by:\n",
        "$$F(\\vec{r}_{p}, \\vec{r}_{t}) = \\frac{1}{2} \\left( 1 + \\vec{r}_{p} \\cdot \\vec{r}_{t} + \\sqrt{(1 - ||\\vec{r}_{p}||^2)(1 - ||\\vec{r}_{t}||^2)} \\right)$$\n",
        "\n",
        "* **Interpretation:** The term $\\vec{r}_{p} \\cdot \\vec{r}_{t}$ aligns the vectors directionally. The term under the square root penalizes errors in **purity** (vector length). This allows the DNN to specifically \"learn\" decoherence.\n",
        "\n",
        "## 3. Architecture Overview\n",
        "\n",
        "### A. Deep Neural Network (DNN - PyTorch)\n",
        "* **Structure:** A Multi-Layer Perceptron (MLP) with fully connected layers and non-linear activation functions (ReLU).\n",
        "* **Why:** To test if a \"Universal Approximator\" can learn the noise models better than fixed kernels.\n",
        "\n",
        "### B. Variational Quantum Circuit (VQC - PennyLane)\n",
        "* **Concept:** We use a parameterized quantum circuit as the model.\n",
        "* **Mechanism:**\n",
        "    1.  **Encoding:** Classical inputs ($X, Y, Z$) are embedded into a quantum state via rotation gates.\n",
        "    2.  **Processing:** A sequence of trainable gates (Ansatz) manipulates the state.\n",
        "    3.  **Measurement:** We measure the expectation values of Pauli operators to obtain the output vector.\n",
        "* **Hypothesis:** \"Quantum for Quantum\". A quantum circuit naturally evolves on the Bloch sphere (or inside it for mixed states via subsystems), which might offer better generalization with fewer parameters.\n",
        "\n",
        "## 4. Implementation: High-Performance Computing (GPU)\n",
        "With SVC, we had quite some long training time. So in order to handle the computational load of training deep networks and simulating quantum circuits, we leverage **GPU Acceleration**:\n",
        "* **PyTorch (CUDA/MPS):** For tensor operations and automatic differentiation of the DNN.\n",
        "* **PennyLane Lightning GPU:** Using high-performance state-vector simulators (like `lightning.gpu` or `lightning.qubit`) to accelerate the VQC simulation and gradient calculation (adjoint differentiation).\n",
        "\n",
        "We also do this as a way to learn modern high-performance ML pipelines.\n",
        "\n",
        "## 5. Input/Output Interfaces\n",
        "To ensure a rigorous comparison with the SVR baseline from Notebook 1, the I/O structure remains identical:\n",
        "\n",
        "* **Input $\\mathbf{X}$:** Noisy measurement expectations $[ \\langle X \\rangle_{noise}, \\langle Y \\rangle_{noise}, \\langle Z \\rangle_{noise} ]$.\n",
        "* **Output $\\mathbf{y}$:** Predicted Bloch vector components $[\\hat{x}, \\hat{y}, \\hat{z}]$.\n",
        "\n",
        "*Note: The predicted vector is implicitly constrained to valid physical states (norm $\\le$ 1) either via activation functions (Tanh) or penalty terms in the loss.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import du générateur de dataset\n",
        "try:\n",
        "    from saint_dtSet import generate_qubit_tomography_dataset_base\n",
        "except ImportError:\n",
        "    from dataset_build.saint_dtSet import generate_qubit_tomography_dataset_base\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "print(f\"Device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fidelity_from_bloch(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calcul analytique de la fidélité entre deux vecteurs de Bloch (qubit).\n",
        "    pred et target: tensors (..., 3).\n",
        "    \"\"\"\n",
        "    dot = (pred * target).sum(dim=-1)\n",
        "    norm_pred = (pred ** 2).sum(dim=-1)\n",
        "    norm_target = (target ** 2).sum(dim=-1)\n",
        "    under_sqrt = torch.clamp(1.0 - norm_pred, min=0.0) * torch.clamp(1.0 - norm_target, min=0.0)\n",
        "    fidelity = 0.5 * (1.0 + dot + torch.sqrt(under_sqrt))\n",
        "    return torch.clamp(fidelity, 0.0, 1.0)\n",
        "\n",
        "\n",
        "def build_dataloaders(df: pd.DataFrame, batch_size: int = 32):\n",
        "    features = df[['X_mean', 'Y_mean', 'Z_mean']].to_numpy(dtype=np.float32)\n",
        "    targets = df[['X_real', 'Y_real', 'Z_real']].to_numpy(dtype=np.float32)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "    train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "    test_ds = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader, (X_test, y_test)\n",
        "\n",
        "\n",
        "def evaluate_model(model, data_loader, device=DEVICE):\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in data_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            pred = model(xb)\n",
        "            preds.append(pred.detach().cpu())\n",
        "            targets.append(yb.detach().cpu())\n",
        "    preds_t = torch.cat(preds, dim=0)\n",
        "    targets_t = torch.cat(targets, dim=0)\n",
        "    fidelities = fidelity_from_bloch(preds_t, targets_t)\n",
        "    return fidelities.mean().item()\n",
        "\n",
        "\n",
        "def compute_baseline_fidelity(x_test: np.ndarray, y_test: np.ndarray) -> float:\n",
        "    # Baseline rapide ~SVR/MLE : prédicteur trivial X_mean->X_real (sans contrainte de sphère).\n",
        "    preds = torch.from_numpy(x_test)\n",
        "    targets = torch.from_numpy(y_test)\n",
        "    return fidelity_from_bloch(preds, targets).mean().item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, optimizer, loss_fn, epochs: int, device=DEVICE):\n",
        "    \"\"\"\n",
        "    Boucle générique d'entraînement.\n",
        "    Déplace inputs/targets sur DEVICE avant forward comme demandé.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    history = []\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = loss_fn(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        history.append(epoch_loss)\n",
        "        if epoch == 1 or epoch % max(1, epochs // 5) == 0:\n",
        "            print(f\"Epoch {epoch:>3}/{epochs} - loss: {epoch_loss:.4f}\")\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Boucle d'expérimentation principale\n",
        "# ---------------------------------------------------------------------------\n",
        "N_STATES = 2000\n",
        "N_SHOTS = 500\n",
        "DECOHERENCE_LEVELS = [0.0, 0.2, 0.5, 0.8]\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_DNN = 25\n",
        "EPOCHS_VQC = 12  # VQC plus lent -> moins d'epochs\n",
        "LR_DNN = 0.01\n",
        "LR_VQC = 0.05\n",
        "\n",
        "results = {\"dnn\": [], \"vqc\": [], \"baseline\": []}\n",
        "\n",
        "for level in DECOHERENCE_LEVELS:\n",
        "    print(f\"\n",
        "=== Décohérence {level} ===\")\n",
        "    df = generate_qubit_tomography_dataset_base(\n",
        "        n_states=N_STATES,\n",
        "        n_shots=N_SHOTS,\n",
        "        include_decoherence=True,\n",
        "        decoherence_level=level,\n",
        "        mode=\"finite_shots\",\n",
        "        random_state=1234\n",
        "    )\n",
        "\n",
        "    train_loader, test_loader, (x_test, y_test) = build_dataloaders(df, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # ---------------------- DNN ----------------------\n",
        "    dnn = TomographyDNN().to(DEVICE)\n",
        "    dnn_optimizer = torch.optim.Adam(dnn.parameters(), lr=LR_DNN)\n",
        "    dnn_loss = QuantumFidelityLoss()\n",
        "    _ = train_model(dnn, train_loader, dnn_optimizer, dnn_loss, epochs=EPOCHS_DNN, device=DEVICE)\n",
        "    dnn_fid = evaluate_model(dnn, test_loader, device=DEVICE)\n",
        "\n",
        "    # ---------------------- VQC ----------------------\n",
        "    vqc = TomographyVQC().to(DEVICE)\n",
        "    vqc_optimizer = torch.optim.Adam(vqc.parameters(), lr=LR_VQC)\n",
        "    vqc_loss = QuantumFidelityLoss()\n",
        "    _ = train_model(vqc, train_loader, vqc_optimizer, vqc_loss, epochs=EPOCHS_VQC, device=DEVICE)\n",
        "    vqc_fid = evaluate_model(vqc, test_loader, device=DEVICE)\n",
        "\n",
        "    # -------------------- Baseline --------------------\n",
        "    baseline_fid = compute_baseline_fidelity(x_test, y_test)\n",
        "\n",
        "    results[\"dnn\"].append(dnn_fid)\n",
        "    results[\"vqc\"].append(vqc_fid)\n",
        "    results[\"baseline\"].append(baseline_fid)\n",
        "\n",
        "    print(f\"DNN fidelity (test): {dnn_fid:.4f}\")\n",
        "    print(f\"VQC fidelity (test): {vqc_fid:.4f}\")\n",
        "    print(f\"Baseline fidelity    : {baseline_fid:.4f}\")\n",
        "\n",
        "print('Boucle terminée.')\n",
        "results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualisation des fidélités moyennes\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(DECOHERENCE_LEVELS, results['dnn'], '-o', label='DNN')\n",
        "plt.plot(DECOHERENCE_LEVELS, results['vqc'], '-o', label='VQC')\n",
        "plt.plot(DECOHERENCE_LEVELS, results['baseline'], '-o', label='Baseline (SVR/MLE rapide)')\n",
        "plt.xlabel('Niveau de décohérence')\n",
        "plt.ylabel('Fidélité moyenne (test)')\n",
        "plt.ylim(0.0, 1.05)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
