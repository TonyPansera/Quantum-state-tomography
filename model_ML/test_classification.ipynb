{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d3972e",
   "metadata": {},
   "source": [
    "L’objectif de cette section est d’évaluer la capacité d’un modèle de Machine Learning à extraire une propriété physique simple d’un état quantique à partir de mesures bruitées. Dans le cadre de la tomographie quantique, on souhaite en général reconstruire l’état réel du qubit, représenté par son vecteur de Bloch (X_real, Y_real, Z_real). Cependant, avant d’aborder cette tâche plus complexe de régression, il est utile de commencer par un problème plus simple : déterminer si l’état appartient à l’hémisphère nord ou à l’hémisphère sud de la sphère de Bloch.\n",
    "\n",
    "Pour cela, on définit une étiquette binaire directement à partir de l’état réel simulé : label = 1 si Z_real ≥ 0, et 0 sinon. Cette propriété géométrique simple découle uniquement de l’état réel, c’est-à-dire l’état effectivement mesuré après bruit statistique et éventuellement bruit de canal. L’objectif de la classification est donc de prédire cette étiquette en utilisant uniquement les mesures brutes bruitées, à savoir les valeurs X_mean, Y_mean et Z_mean obtenues à partir d’un nombre limité de shots.\n",
    "\n",
    "Ce test sert de validation préalable essentielle. S’il est impossible pour un modèle de classification de récupérer une propriété globale aussi simple que le signe de Z_real, il est illusoire d’espérer réussir une reconstruction complète du vecteur de Bloch réel via une régression plus fine. À l’inverse, si la classification réussit malgré le bruit statistique, cela montre que les features contiennent bien une partie exploitable de l’information physique.\n",
    "\n",
    "En résumé, cette classification n’a pas pour but de résoudre la tomographie complète, mais de tester de manière contrôlée et progressive la capacité d’un modèle à extraire une information physique réelle à partir de mesures bruitées. Ce type d’analyse constitue une étape préparatoire logique avant de s’engager dans des tâches de reconstruction plus précises de l’état quantique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3424431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Imports Qiskit\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit_machine_learning.algorithms import QSVC\n",
    "\n",
    "from dataset_build.clf_dtSet_hugo import build_purity_classification_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb00551",
   "metadata": {},
   "source": [
    "We can use the Strong law of numbers to prepare the n_shots value. Using graphical plots to see the convergence of the random average variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28a08f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (500, 14)\n",
      "Labels distribution:\n",
      "label_purity\n",
      "1    350\n",
      "0    150\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset columns:\n",
      "['X_mean', 'Y_mean', 'Z_mean', 'X_real', 'Y_real', 'Z_real', 'theta_ideal', 'phi_ideal', 'X_ideal', 'Y_ideal', 'Z_ideal', 'bloch_radius_real', 'is_pure', 'label_purity']\n",
      "\\X columns (à revoir):\n",
      "['X_mean', 'Y_mean', 'Z_mean', 'bloch_radius_real']\n"
     ]
    }
   ],
   "source": [
    "# Dataset creation\n",
    "\n",
    "df_purity, X, y = build_purity_classification_dataset(\n",
    "    n_shots=100,\n",
    "    n_states_total=500,\n",
    "    mixed_proportion=0.3  # proportion of mixed states\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {df_purity.shape}\")\n",
    "print(f\"Labels distribution:\\n{y.value_counts()}\")\n",
    "print(f\"\\nDataset columns:\\n{df_purity.columns.tolist()}\")\n",
    "print(f\"\\X columns (à revoir):\\n{X.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2fbc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purity.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purity.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f535e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purity.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42046333",
   "metadata": {},
   "source": [
    "The X,Y and Z are following a unimodal Gaussian distribution, so that the dataset creation is normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1d910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df_purity.corr(), annot=True, cmap='coolwarm', fmt='.2f', cbar_kws={\"shrink\": .8})\n",
    "plt.title('Features correlation matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a0f9c4",
   "metadata": {},
   "source": [
    "Putting bloch_radius_real creates an overfitting training, so we take it out from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28675ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_purity=df_purity[['X_mean','Y_mean','Z_mean']]\n",
    "X_purity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac35fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_purity['label_purity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8074e",
   "metadata": {},
   "source": [
    "Transforming data for training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff05af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "\n",
    "\n",
    "X_purity_scaled=scaler.fit_transform(X_purity)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_purity_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7f67d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Features: exemple simple (tu peux changer)\n",
    "feature_cols = [\n",
    "    \"X_mean\", \"Y_mean\", \"Z_mean\",\n",
    "    \"theta_ideal\", \"phi_ideal\",\n",
    "    \"bloch_radius_real\"\n",
    "]\n",
    "X = df_purity[feature_cols].values\n",
    "\n",
    "# 3) Labels\n",
    "y = df_purity[\"label_purity\"].values  # ou df[\"is_pure\"].astype(int).values\n",
    "\n",
    "# 4) Train / test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler_enc = MinMaxScaler(feature_range=(0, np.pi))\n",
    "\n",
    "X_train_enc = scaler_enc.fit_transform(X_train)\n",
    "X_test_enc  = scaler_enc.transform(X_test)\n",
    "\n",
    "num_features = X_train_enc.shape[1]\n",
    "\n",
    "feature_map = ZZFeatureMap(\n",
    "    feature_dimension=num_features,\n",
    "    reps=2,                 # profondeur entanglement, à tuner\n",
    "    entanglement=\"full\"     # ou \"linear\"\n",
    ")\n",
    "\n",
    "quantum_kernel = FidelityQuantumKernel(\n",
    "    feature_map=feature_map\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6f3dd2",
   "metadata": {},
   "source": [
    "Training SVC models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "580fc3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QSVC - train acc: 0.995, test acc: 0.970\n"
     ]
    }
   ],
   "source": [
    "qsvc = QSVC(quantum_kernel=quantum_kernel, C=1.0)\n",
    "\n",
    "qsvc.fit(X_train_enc, y_train)\n",
    "train_acc_qsvc = qsvc.score(X_train_enc, y_train)\n",
    "test_acc_qsvc  = qsvc.score(X_test_enc, y_test)\n",
    "print(f\"QSVC - train acc: {train_acc_qsvc:.3f}, test acc: {test_acc_qsvc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd980c9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m K_train = \u001b[43mquantum_kernel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m K_test  = quantum_kernel.evaluate(X_test_enc,  X_train_enc)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 2) SVM classique sur noyau pré-calculé\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tonyp\\Documents\\DeVInci\\Quantum research\\venv\\Lib\\site-packages\\qiskit_machine_learning\\kernels\\fidelity_quantum_kernel.py:114\u001b[39m, in \u001b[36mFidelityQuantumKernel.evaluate\u001b[39m\u001b[34m(self, x_vec, y_vec)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_symmetric:\n\u001b[32m    113\u001b[39m     left_parameters, right_parameters, indices = \u001b[38;5;28mself\u001b[39m._get_symmetric_parameterization(x_vec)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     kernel_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_symmetric_kernel_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkernel_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    118\u001b[39m     left_parameters, right_parameters, indices = \u001b[38;5;28mself\u001b[39m._get_parameterization(x_vec, y_vec)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tonyp\\Documents\\DeVInci\\Quantum research\\venv\\Lib\\site-packages\\qiskit_machine_learning\\kernels\\fidelity_quantum_kernel.py:208\u001b[39m, in \u001b[36mFidelityQuantumKernel._get_symmetric_kernel_matrix\u001b[39m\u001b[34m(self, kernel_shape, left_parameters, right_parameters, indices)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_symmetric_kernel_matrix\u001b[39m(\n\u001b[32m    199\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    200\u001b[39m     kernel_shape: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    203\u001b[39m     indices: KernelIndices,\n\u001b[32m    204\u001b[39m ) -> np.ndarray:\n\u001b[32m    205\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[33;03m    Given a set of parameterization, this computes the kernel matrix.\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     kernel_entries = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_kernel_entries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m     kernel_matrix = np.ones(kernel_shape)\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, (col, row) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(indices):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tonyp\\Documents\\DeVInci\\Quantum research\\venv\\Lib\\site-packages\\qiskit_machine_learning\\kernels\\fidelity_quantum_kernel.py:235\u001b[39m, in \u001b[36mFidelityQuantumKernel._get_kernel_entries\u001b[39m\u001b[34m(self, left_parameters, right_parameters)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.max_circuits_per_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    229\u001b[39m     job = \u001b[38;5;28mself\u001b[39m._fidelity.run(\n\u001b[32m    230\u001b[39m         [\u001b[38;5;28mself\u001b[39m._feature_map] * num_circuits,\n\u001b[32m    231\u001b[39m         [\u001b[38;5;28mself\u001b[39m._feature_map] * num_circuits,\n\u001b[32m    232\u001b[39m         left_parameters,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    233\u001b[39m         right_parameters,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    234\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     kernel_entries = \u001b[43mjob\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.fidelities\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    237\u001b[39m     \u001b[38;5;66;03m# Determine the number of chunks needed\u001b[39;00m\n\u001b[32m    238\u001b[39m     num_chunks = (\n\u001b[32m    239\u001b[39m         num_circuits + \u001b[38;5;28mself\u001b[39m.max_circuits_per_job - \u001b[32m1\u001b[39m\n\u001b[32m    240\u001b[39m     ) // \u001b[38;5;28mself\u001b[39m.max_circuits_per_job\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tonyp\\Documents\\DeVInci\\Quantum research\\venv\\Lib\\site-packages\\qiskit\\primitives\\primitive_job.py:51\u001b[39m, in \u001b[36mPrimitiveJob.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ResultT:\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_submitted()\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_future\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "K_train = quantum_kernel.evaluate(X_train_enc, X_train_enc)\n",
    "K_test  = quantum_kernel.evaluate(X_test_enc,  X_train_enc)\n",
    "\n",
    "# 2) SVM classique sur noyau pré-calculé\n",
    "svc_q = SVC(kernel=\"precomputed\", C=1.0)\n",
    "svc_q.fit(K_train, y_train)\n",
    "\n",
    "# 3) Prédictions SANS recalcul de kernel\n",
    "y_train_pred = svc_q.predict(K_train)\n",
    "y_test_pred  = svc_q.predict(K_test)\n",
    "\n",
    "print(\"\\n=== Classification report (test) ===\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c1dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svc,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1  # utiliser tous les processeurs disponibles\n",
    ")\n",
    "\n",
    "# Entraîner le modèle avec recherche en grille\n",
    "print(\"Démarrage de GridSearchCV...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nMeilleurs paramètres : {grid_search.best_params_}\")\n",
    "print(f\"Meilleur score CV : {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluating on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracité sur l'ensemble de test : {test_accuracy:.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2964f710",
   "metadata": {},
   "source": [
    "without parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92155f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SVC=SVC(random_state=42)\n",
    "model_SVC.fit(X_train, y_train)\n",
    "y_pred_SVC=model_SVC.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_SVC))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec37c8cd",
   "metadata": {},
   "source": [
    "Impact of n_shots on the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22daf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser l'impact de n_shots sur la performance du modèle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "n_shots_values = [10, 50, 100, 500, 1000, 5000, 10000]\n",
    "results = []\n",
    "\n",
    "print(\"Analyse de l'impact de n_shots sur la performance du modèle (F1-Score)...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for n_shots in n_shots_values:\n",
    "    # Générer dataset avec ce n_shots\n",
    "    df_temp, X_temp, y_temp = build_purity_classification_dataset(\n",
    "        n_shots=n_shots,\n",
    "        n_states_total=5000,\n",
    "        mixed_proportion=0.3\n",
    "    )\n",
    "    \n",
    "    # Préparer les données\n",
    "    X_temp_data = df_temp[['X_mean', 'Y_mean', 'Z_mean']]\n",
    "    X_temp_scaled = StandardScaler().fit_transform(X_temp_data)\n",
    "    \n",
    "    # Évaluer avec le modèle optimal trouvé avec F1-Score\n",
    "    model = SVC(**grid_search.best_params_)\n",
    "    cv_scores = cross_val_score(model, X_temp_scaled, y_temp, cv=5, scoring='f1_weighted')\n",
    "    \n",
    "    results.append({\n",
    "        'n_shots': n_shots,\n",
    "        'mean_f1': cv_scores.mean(),\n",
    "        'std_f1': cv_scores.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"n_shots={n_shots:5d} | F1-Score: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Créer DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Visualiser la convergence avec F1-Score\n",
    "fig, ax = plt.subplots(figsize=(11, 6))\n",
    "\n",
    "ax.errorbar(results_df['n_shots'], results_df['mean_f1'], \n",
    "            yerr=results_df['std_f1'], \n",
    "            fmt='o-', capsize=5, linewidth=2.5, markersize=10, \n",
    "            color='darkblue', ecolor='lightblue', label='F1-Score ± Écart-type')\n",
    "\n",
    "ax.set_xlabel('n_shots (nombre de mesures)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('F1-Score (5-fold CV)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Impact de n_shots sur la performance du modèle (F1-Score pondéré)', fontsize=14, fontweight='bold')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=11, loc='lower right')\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Recommandation\n",
    "optimal_idx = results_df['mean_f1'].idxmax()\n",
    "optimal_n_shots = results_df.loc[optimal_idx, 'n_shots']\n",
    "optimal_f1 = results_df.loc[optimal_idx, 'mean_f1']\n",
    "optimal_std = results_df.loc[optimal_idx, 'std_f1']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RÉSUMÉ - IMPACT DE N_SHOTS SUR LA PERFORMANCE (F1-SCORE)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\n✓ n_shots optimal : {int(optimal_n_shots)}\")\n",
    "print(f\"  F1-Score: {optimal_f1:.4f} ± {optimal_std:.4f}\")\n",
    "print(f\"\\n✓ Progression du F1-Score:\")\n",
    "print(f\"  • n_shots minimal (10):     F1 = {results_df.loc[0, 'mean_f1']:.4f}\")\n",
    "print(f\"  • n_shots optimal ({int(optimal_n_shots)}):  F1 = {optimal_f1:.4f} ⭐\")\n",
    "print(f\"  • n_shots maximal (10000):  F1 = {results_df.loc[len(results_df)-1, 'mean_f1']:.4f}\")\n",
    "print(f\"\\n✓ Amélioration : {(optimal_f1 - results_df.loc[0, 'mean_f1'])*100:.2f}% par rapport à n_shots=10\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053823b5",
   "metadata": {},
   "source": [
    "Convergence des (X_mean, Y_mean, Z_mean) selon le nombre de shots en 3D et 2D (avec loi des grands nombres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78285b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection 3D des mesures (X_mean, Y_mean, Z_mean) selon n_shots\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "n_shots_test = [10, 100, 1000, 5000]\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "print(\"Génération des projections 3D pour différentes valeurs de n_shots...\\n\")\n",
    "\n",
    "for idx, n_shots in enumerate(n_shots_test, 1):\n",
    "    # Générer dataset\n",
    "    df_temp, X_temp, y_temp = build_purity_classification_dataset(\n",
    "        n_shots=n_shots,\n",
    "        n_states_total=1000,\n",
    "        mixed_proportion=0.3\n",
    "    )\n",
    "    \n",
    "    # Sous-graphique 3D\n",
    "    ax = fig.add_subplot(2, 2, idx, projection='3d')\n",
    "    \n",
    "    # Séparer les états purs et mixtes\n",
    "    pure_mask = y_temp == 1\n",
    "    mixed_mask = y_temp == 0\n",
    "    \n",
    "    # Scatter plot des états purs et mixtes\n",
    "    ax.scatter(df_temp.loc[pure_mask, 'X_mean'],\n",
    "               df_temp.loc[pure_mask, 'Y_mean'],\n",
    "               df_temp.loc[pure_mask, 'Z_mean'],\n",
    "               c='blue', marker='o', s=30, alpha=0.6, label='États purs')\n",
    "    \n",
    "    ax.scatter(df_temp.loc[mixed_mask, 'X_mean'],\n",
    "               df_temp.loc[mixed_mask, 'Y_mean'],\n",
    "               df_temp.loc[mixed_mask, 'Z_mean'],\n",
    "               c='red', marker='x', s=50, alpha=0.6, label='États mixtes')\n",
    "    \n",
    "    # Ajouter la sphère de Bloch (rayon = 1)\n",
    "    u = np.linspace(0, 2 * np.pi, 30)\n",
    "    v = np.linspace(0, np.pi, 20)\n",
    "    x_sphere = np.outer(np.cos(u), np.sin(v))\n",
    "    y_sphere = np.outer(np.sin(u), np.sin(v))\n",
    "    z_sphere = np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "    ax.plot_surface(x_sphere, y_sphere, z_sphere, alpha=0.1, color='gray')\n",
    "    \n",
    "    # Calcul des statistiques\n",
    "    mean_radius = np.sqrt(df_temp['X_mean']**2 + df_temp['Y_mean']**2 + df_temp['Z_mean']**2).mean()\n",
    "    std_radius = np.sqrt(df_temp['X_mean']**2 + df_temp['Y_mean']**2 + df_temp['Z_mean']**2).std()\n",
    "    \n",
    "    # Labels et titre\n",
    "    ax.set_xlabel('X_mean', fontsize=10, fontweight='bold')\n",
    "    ax.set_ylabel('Y_mean', fontsize=10, fontweight='bold')\n",
    "    ax.set_zlabel('Z_mean', fontsize=10, fontweight='bold')\n",
    "    ax.set_title(f'n_shots = {n_shots}\\n(Rayon moyen: {mean_radius:.3f} ± {std_radius:.3f})', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    \n",
    "    # Fixer les limites\n",
    "    ax.set_xlim([-1.2, 1.2])\n",
    "    ax.set_ylim([-1.2, 1.2])\n",
    "    ax.set_zlim([-1.2, 1.2])\n",
    "    \n",
    "    print(f\"n_shots={n_shots:5d} | Rayon moyen: {mean_radius:.4f} ± {std_radius:.4f}\")\n",
    "\n",
    "plt.suptitle('Projection 3D des mesures (X_mean, Y_mean, Z_mean) selon n_shots', \n",
    "             fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRÉTATION:\")\n",
    "print(\"=\"*80)\n",
    "print(\"• Sphère grise: Sphère de Bloch idéale (rayon = 1)\")\n",
    "print(\"• Points bleus (o): États purs (devraient être près de la sphère)\")\n",
    "print(\"• Points rouges (x): États mixtes (devraient être à l'intérieur de la sphère)\")\n",
    "print(\"• Plus n_shots augmente, plus les mesures se rapprochent de la vraie géométrie\")\n",
    "print(\"• Avec n_shots faible: plus de bruit, points très dispersés\")\n",
    "print(\"• Avec n_shots élevé: mesures plus précises, distribution plus concentrée\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3126bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage des mesures (X_real, Y_real, Z_real) sur la sphère de Bloch \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6321d2ef",
   "metadata": {},
   "source": [
    "LFGN to choose n_shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af48ba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence de la distance entre état mesuré et état réel selon n_shots\n",
    "# Application de la Loi des Grands Nombres\n",
    "\n",
    "n_shots_values = [10, 25, 50, 100, 250, 500, 1000, 2500, 5000]\n",
    "distance_stats = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ANALYSE DE CONVERGENCE: Distance mesure-réalité vs n_shots\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nConcept: La Loi des Grands Nombres prédit que l'erreur d'estimation\")\n",
    "print(\"décroît comme 1/√n_shots pour les moyennes statistiques.\\n\")\n",
    "\n",
    "for n_shots in n_shots_values:\n",
    "    # Générer dataset\n",
    "    df_temp, X_temp, y_temp = build_purity_classification_dataset(\n",
    "        n_shots=n_shots,\n",
    "        n_states_total=5000,\n",
    "        mixed_proportion=0.5\n",
    "    )\n",
    "    \n",
    "    # Calculer la distance Euclidienne entre (X_mean, Y_mean, Z_mean) et (X_real, Y_real, Z_real)\n",
    "    distances = np.sqrt(\n",
    "        (df_temp['X_mean'] - df_temp['X_real'])**2 +\n",
    "        (df_temp['Y_mean'] - df_temp['Y_real'])**2 +\n",
    "        (df_temp['Z_mean'] - df_temp['Z_real'])**2\n",
    "    )\n",
    "    \n",
    "    # Statistiques\n",
    "    mean_distance = distances.mean()\n",
    "    std_distance = distances.std()\n",
    "    \n",
    "    distance_stats.append({\n",
    "        'n_shots': n_shots,\n",
    "        'mean_distance': mean_distance,\n",
    "        'std_distance': std_distance\n",
    "    })\n",
    "    \n",
    "    print(f\"n_shots={n_shots:5d} | Distance: {mean_distance:.6f} ± {std_distance:.6f}\")\n",
    "\n",
    "# Créer DataFrame\n",
    "distance_df = pd.DataFrame(distance_stats)\n",
    "\n",
    "# Visualisation: 2 graphiques\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Graphique 1: Distance moyenne avec erreur\n",
    "ax1 = axes[0]\n",
    "ax1.errorbar(distance_df['n_shots'], distance_df['mean_distance'],\n",
    "             yerr=distance_df['std_distance'],\n",
    "             fmt='o-', capsize=5, linewidth=2.5, markersize=8,\n",
    "             color='darkblue', ecolor='lightblue', label='Moyenne ± Écart-type')\n",
    "ax1.axhline(y=distance_df['mean_distance'].iloc[-1], color='green', linestyle='--',\n",
    "            linewidth=2, label=f'Distance convergée (n_shots=5000)', alpha=0.7)\n",
    "ax1.set_xlabel('n_shots', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Distance Euclidienne', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Distance Mesure-Réalité vs n_shots', fontsize=13, fontweight='bold')\n",
    "ax1.set_xscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Trouver le point d'inflexion (diminution < 10%)\n",
    "distances = distance_df['mean_distance'].values\n",
    "initial_distance = distances[0]\n",
    "\n",
    "for i in range(1, len(distances)):\n",
    "    reduction = (initial_distance - distances[i]) / initial_distance * 100\n",
    "    if i > 1:\n",
    "        prev_reduction = (initial_distance - distances[i-1]) / initial_distance * 100\n",
    "        improvement = reduction - prev_reduction\n",
    "        if improvement < 5:  # Amélioration inférieure à 5%\n",
    "            optimal_idx = i - 1\n",
    "            break\n",
    "else:\n",
    "    optimal_idx = len(distance_df) - 1\n",
    "\n",
    "optimal_n_shots = distance_df.loc[optimal_idx, 'n_shots']\n",
    "optimal_distance = distance_df.loc[optimal_idx, 'mean_distance']\n",
    "optimal_reduction = (initial_distance - optimal_distance) / initial_distance * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba8b53",
   "metadata": {},
   "source": [
    "optimal n_shots = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d40a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset creation\n",
    "\n",
    "df_purity_5000, X, y = build_purity_classification_dataset(\n",
    "    n_shots=5000,\n",
    "    n_states_total=5000,\n",
    "    mixed_proportion=0.5  # proportion of mixed states\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {df_purity_5000.shape}\")\n",
    "print(f\"Labels distribution:\\n{y.value_counts()}\")\n",
    "print(f\"\\nDataset columns:\\n{df_purity_5000.columns.tolist()}\")\n",
    "print(f\"\\X columns (à revoir):\\n{X.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5866c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purity_5000.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b603f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purity_5000.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2044d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_purity_5000=df_purity_5000[['X_mean','Y_mean','Z_mean']]\n",
    "y_purity_5000=df_purity_5000['label_purity']\n",
    "\n",
    "# standardization\n",
    "X_purity_5000_scaled=scaler.fit_transform(X_purity_5000)\n",
    "\n",
    "#splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_purity_5000_scaled, y_purity_5000, test_size=0.2, random_state=42, stratify=y_purity_5000)\n",
    "\n",
    "# selecting the best parameters from the grid search\n",
    "model_5000 = SVC(**grid_search.best_params_)\n",
    "model_5000.fit(X_train, y_train)\n",
    "y_pred = model_5000.predict(X_test)  \n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006fa817",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_poly_2=SVC(C= 10, gamma= 'scale', kernel= 'poly', degree=2)\n",
    "model_poly_2.fit(X_train, y_train)\n",
    "y_pred_poly_2=model_poly_2.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_poly_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ce0c2",
   "metadata": {},
   "source": [
    "Plotting decision boundaries for the best models : Gaussian and degree 2 poly kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfdc09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des decision boundaries en 2D: RBF vs Polynomial (plan X_mean, Z_mean)\n",
    "\n",
    "y_train_labels = y_train.values\n",
    "y_test_labels = y_test.values\n",
    "\n",
    "# Créer une grille 2D fine sur le plan (X, Z) - axes 0 et 2\n",
    "h = 0.05  # pas de la grille pour 2D (plus fin que 3D)\n",
    "x_min, x_max = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5\n",
    "z_min, z_max = X_train[:, 2].min() - 0.5, X_train[:, 2].max() + 0.5\n",
    "\n",
    "xx_2d, zz_2d = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                            np.arange(z_min, z_max, h))\n",
    "\n",
    "# Pour les prédictions, il faut passer 3 features (X, Y, Z)\n",
    "# On va utiliser la moyenne Y (0) pour les points de grille\n",
    "y_mean_2d = np.zeros_like(xx_2d)\n",
    "\n",
    "# Créer les points de grille avec Y = 0 (plan central)\n",
    "# Format: [X, Y, Z]\n",
    "grid_points = np.c_[xx_2d.ravel(), y_mean_2d.ravel(), zz_2d.ravel()]\n",
    "\n",
    "# Prédictions sur la grille pour les deux modèles\n",
    "Z_rbf_2d = model_5000.predict(grid_points)\n",
    "Z_poly_2d = model_poly_2.predict(grid_points)\n",
    "\n",
    "# Reshape pour contourf\n",
    "Z_rbf_2d = Z_rbf_2d.reshape(xx_2d.shape)\n",
    "Z_poly_2d = Z_poly_2d.reshape(xx_2d.shape)\n",
    "\n",
    "# Créer la figure avec deux subplots 2D\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Subplot 1: RBF Kernel\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Afficher la surface de décision RBF avec contourf\n",
    "contour_rbf = ax1.contourf(xx_2d, zz_2d, Z_rbf_2d, levels=10, cmap='RdYlBu_r', alpha=0.6)\n",
    "ax1.contour(xx_2d, zz_2d, Z_rbf_2d, levels=[0.5], colors='black', linewidths=2, label='Decision Boundary')\n",
    "\n",
    "# Points d'entraînement (X et Z)\n",
    "ax1.scatter(X_train[y_train_labels == 1, 0], X_train[y_train_labels == 1, 2],\n",
    "           c='blue', marker='o', s=50, alpha=0.7, edgecolors='darkblue', linewidth=1, label='Train classe 1')\n",
    "ax1.scatter(X_train[y_train_labels == 0, 0], X_train[y_train_labels == 0, 2],\n",
    "           c='red', marker='x', s=100, alpha=0.7, linewidth=2, label='Train classe 0')\n",
    "\n",
    "# Points de test (X et Z)\n",
    "ax1.scatter(X_test[y_test_labels == 1, 0], X_test[y_test_labels == 1, 2],\n",
    "           c='darkblue', marker='^', s=80, alpha=0.9, edgecolors='black', linewidth=1, label='Test classe 1')\n",
    "ax1.scatter(X_test[y_test_labels == 0, 0], X_test[y_test_labels == 0, 2],\n",
    "           c='darkred', marker='s', s=80, alpha=0.9, edgecolors='black', linewidth=1, label='Test classe 0')\n",
    "\n",
    "ax1.set_xlim([x_min, x_max])\n",
    "ax1.set_ylim([z_min, z_max])\n",
    "ax1.set_xlabel('X_mean (standardisé)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Z_mean (standardisé)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Decision Boundary - RBF Kernel (GridSearch)', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10, loc='best')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "plt.colorbar(contour_rbf, ax=ax1, label='Probabilité classe 1')\n",
    "\n",
    "# Subplot 2: Polynomial Kernel (degree=2)\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Afficher la surface de décision Polynomial avec contourf\n",
    "contour_poly = ax2.contourf(xx_2d, zz_2d, Z_poly_2d, levels=10, cmap='RdYlBu_r', alpha=0.6)\n",
    "ax2.contour(xx_2d, zz_2d, Z_poly_2d, levels=[0.5], colors='black', linewidths=2, label='Decision Boundary')\n",
    "\n",
    "# Points d'entraînement (X et Z)\n",
    "ax2.scatter(X_train[y_train_labels == 1, 0], X_train[y_train_labels == 1, 2],\n",
    "           c='blue', marker='o', s=50, alpha=0.7, edgecolors='darkblue', linewidth=1, label='Train classe 1')\n",
    "ax2.scatter(X_train[y_train_labels == 0, 0], X_train[y_train_labels == 0, 2],\n",
    "           c='red', marker='x', s=100, alpha=0.7, linewidth=2, label='Train classe 0')\n",
    "\n",
    "# Points de test (X et Z)\n",
    "ax2.scatter(X_test[y_test_labels == 1, 0], X_test[y_test_labels == 1, 2],\n",
    "           c='darkblue', marker='^', s=80, alpha=0.9, edgecolors='black', linewidth=1, label='Test classe 1')\n",
    "ax2.scatter(X_test[y_test_labels == 0, 0], X_test[y_test_labels == 0, 2],\n",
    "           c='darkred', marker='s', s=80, alpha=0.9, edgecolors='black', linewidth=1, label='Test classe 0')\n",
    "\n",
    "ax2.set_xlim([x_min, x_max])\n",
    "ax2.set_ylim([z_min, z_max])\n",
    "ax2.set_xlabel('X_mean (standardisé)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Z_mean (standardisé)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Decision Boundary - Polynomial (degree=2)', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=10, loc='best')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.colorbar(contour_poly, ax=ax2, label='Probabilité classe 1')\n",
    "\n",
    "plt.suptitle('Comparaison des Decision Boundaries: RBF vs Polynomial (Plan X_mean, Z_mean)', \n",
    "             fontsize=15, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Afficher les statistiques de comparaison\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARAISON DES KERNELS - DECISION BOUNDARIES EN 2D (X, Z)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prédictions pour évaluation sur le plan 2D avec Y=0\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "y_pred_rbf_2d = model_5000.predict(X_test)\n",
    "y_pred_poly_2d = model_poly_2.predict(X_test)\n",
    "\n",
    "# Utiliser le F1-score pondéré\n",
    "rbf_f1_score_2d = f1_score(y_test_labels, y_pred_rbf_2d, average='weighted')\n",
    "poly_f1_score_2d = f1_score(y_test_labels, y_pred_poly_2d, average='weighted')\n",
    "\n",
    "print(f\"\\n✓ RBF Kernel (GridSearch):\")\n",
    "print(f\"  Paramètres: {grid_search.best_params_}\")\n",
    "print(f\"  F1-Score (weighted) test (3D complet): {rbf_f1_score_2d:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Polynomial Kernel (degree=2):\")\n",
    "print(f\"  Paramètres: C=10, gamma='scale', kernel='poly', degree=2\")\n",
    "print(f\"  F1-Score (weighted) test (3D complet): {poly_f1_score_2d:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Différence de F1-Score (weighted): {abs(rbf_f1_score_2d - poly_f1_score_2d):.4f}\")\n",
    "print(f\"  → RBF est meilleur: {rbf_f1_score_2d > poly_f1_score_2d}\")\n",
    "\n",
    "print(\"\\n✓ Visualisation 2D (Plan X_mean, Z_mean avec Y=0):\")\n",
    "print(f\"  • Fond coloré: Région de décision (bleu=classe 1, rouge=classe 0)\")\n",
    "print(f\"  • Ligne noire: Frontière de décision (boundary)\")\n",
    "print(f\"  • Cercles bleus (o): Points d'entraînement classe 1\")\n",
    "print(f\"  • Croix rouges (x): Points d'entraînement classe 0\")\n",
    "print(f\"  • Triangles bleus (^): Points de test classe 1\")\n",
    "print(f\"  • Carrés rouges (s): Points de test classe 0\")\n",
    "print(f\"  • Plan XZ avec Y=0 (coupe centrale de la sphère de Bloch)\")\n",
    "print(f\"  • Le RBF crée des régions courbes (non-linéaires)\")\n",
    "print(f\"  • Le Polynomial crée des régions facettées (quadratiques)\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee5f99e",
   "metadata": {},
   "source": [
    "GridSearch: Optimisation conjointe de n_shots et des paramètres SVC pour le meilleur F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ced2ef",
   "metadata": {},
   "source": [
    "For balanced dataset : mixed_proportion=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c365fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch pour optimiser n_shots ET paramètres SVC\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import time\n",
    "\n",
    "# Paramètres à optimiser\n",
    "n_shots_values = [10, 50, 100, 500, 1000, 5000]\n",
    "svc_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],  # Pas de 'poly' par défaut\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'degree': [2]  # Paramètre de degré pour polynomial\n",
    "}\n",
    "\n",
    "# Créer la grille des paramètres\n",
    "svc_param_grid = list(ParameterGrid(svc_params))\n",
    "\n",
    "# Ajouter manuellement le kernel polynomial de degré 2\n",
    "svc_param_grid_extended = []\n",
    "for params in svc_param_grid:\n",
    "    svc_param_grid_extended.append(params)\n",
    "\n",
    "# Ajouter les combinaisons avec polynomial de degré 2\n",
    "for C in [0.1, 1, 10]:\n",
    "    for gamma in ['scale', 'auto']:\n",
    "        svc_param_grid_extended.append({\n",
    "            'C': C,\n",
    "            'kernel': 'poly',\n",
    "            'gamma': gamma,\n",
    "            'degree': 2\n",
    "        })\n",
    "\n",
    "svc_param_grid = svc_param_grid_extended\n",
    "\n",
    "# Résultats\n",
    "results_grid = []\n",
    "total_combinations = len(n_shots_values) * len(svc_param_grid)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"GRIDSEARCH: OPTIMISATION CONJOINTE DE n_shots ET PARAMÈTRES SVC\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nParamètres SVC: {len(svc_param_grid)} combinaisons\")\n",
    "print(f\"Valeurs n_shots: {len(n_shots_values)} valeurs\")\n",
    "print(f\"Total d'itérations: {total_combinations}\\n\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  • n_states_total = 5000\")\n",
    "print(f\"  • mixed_proportion = 0.5\")\n",
    "print(f\"  • Kernels: linear, rbf, poly(degree=2)\")\n",
    "print(f\"  • Métrique: F1-Score (weighted)\")\n",
    "print(f\"  • Cross-validation: 5-fold\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "iteration = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for n_shots in n_shots_values:\n",
    "    print(f\"\\n[n_shots = {n_shots:5d}] Génération dataset et GridSearch SVC...\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Générer le dataset avec ce n_shots\n",
    "    df_temp, _, y_temp = build_purity_classification_dataset(\n",
    "        n_shots=n_shots,\n",
    "        n_states_total=5000,\n",
    "        mixed_proportion=0.5\n",
    "    )\n",
    "    \n",
    "    X_temp_data = df_temp[['X_mean', 'Y_mean', 'Z_mean']]\n",
    "    X_temp_scaled = StandardScaler().fit_transform(X_temp_data)\n",
    "    \n",
    "    # Tester chaque combinaison de paramètres SVC\n",
    "    best_f1_for_n_shots = 0\n",
    "    best_config_for_n_shots = None\n",
    "    \n",
    "    for params in svc_param_grid:\n",
    "        iteration += 1\n",
    "        \n",
    "        # Créer et évaluer le modèle\n",
    "        svc_model = SVC(**params)\n",
    "        \n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        cv_scores = cross_val_score(\n",
    "            svc_model, X_temp_scaled, y_temp, \n",
    "            cv=5, scoring='f1_weighted'\n",
    "        )\n",
    "        \n",
    "        # Enregistrer les résultats\n",
    "        result = {\n",
    "            'n_shots': n_shots,\n",
    "            'C': params['C'],\n",
    "            'kernel': params['kernel'],\n",
    "            'gamma': params['gamma'],\n",
    "            'degree': params.get('degree', None),\n",
    "            'mean_f1': cv_scores.mean(),\n",
    "            'std_f1': cv_scores.std()\n",
    "        }\n",
    "        results_grid.append(result)\n",
    "        \n",
    "        # Afficher chaque exécution\n",
    "        degree_str = f\", degree={params.get('degree', 'N/A')}\" if params['kernel'] == 'poly' else \"\"\n",
    "        print(f\"  n_shots={n_shots:5d} | C={params['C']:4.1f} | kernel={params['kernel']:6s} | gamma={params['gamma']:6s}{degree_str} | F1={cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "        \n",
    "        # Tracker le meilleur pour ce n_shots\n",
    "        if cv_scores.mean() > best_f1_for_n_shots:\n",
    "            best_f1_for_n_shots = cv_scores.mean()\n",
    "            best_config_for_n_shots = result\n",
    "    \n",
    "    # Afficher le meilleur pour ce n_shots\n",
    "    print(f\"  → MEILLEUR POUR n_shots={n_shots}: F1={best_f1_for_n_shots:.4f} (C={best_config_for_n_shots['C']}, kernel={best_config_for_n_shots['kernel']}, gamma={best_config_for_n_shots['gamma']})\")\n",
    "\n",
    "# Créer un DataFrame avec tous les résultats\n",
    "results_grid_df = pd.DataFrame(results_grid)\n",
    "\n",
    "# Trouver le meilleur résultat global\n",
    "best_idx = results_grid_df['mean_f1'].idxmax()\n",
    "best_result = results_grid_df.loc[best_idx]\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"RÉSULTATS DU GRIDSEARCH - MEILLEURE CONFIGURATION GLOBALE\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\n✓ MEILLEUR n_shots: {int(best_result['n_shots'])}\")\n",
    "print(f\"\\n✓ MEILLEURS PARAMÈTRES SVC:\")\n",
    "print(f\"  • C: {best_result['C']}\")\n",
    "print(f\"  • kernel: {best_result['kernel']}\")\n",
    "print(f\"  • gamma: {best_result['gamma']}\")\n",
    "if best_result['degree'] is not None and best_result['kernel'] == 'poly':\n",
    "    print(f\"  • degree: {int(best_result['degree'])}\")\n",
    "print(f\"\\n✓ F1-Score (weighted): {best_result['mean_f1']:.4f} ± {best_result['std_f1']:.4f}\")\n",
    "print(f\"\\n✓ Temps d'exécution total: {elapsed_time:.2f} secondes\")\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab917e",
   "metadata": {},
   "source": [
    "for mixed proportion = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a491b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "import time\n",
    "\n",
    "# Paramètres à optimiser\n",
    "n_shots_values = [10, 50, 100, 500, 1000, 5000]\n",
    "svc_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],  # Pas de 'poly' par défaut\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'degree': [2]  # Paramètre de degré pour polynomial\n",
    "}\n",
    "\n",
    "# Créer la grille des paramètres\n",
    "svc_param_grid = list(ParameterGrid(svc_params))\n",
    "\n",
    "# Ajouter manuellement le kernel polynomial de degré 2\n",
    "svc_param_grid_extended = []\n",
    "for params in svc_param_grid:\n",
    "    svc_param_grid_extended.append(params)\n",
    "\n",
    "# Ajouter les combinaisons avec polynomial de degré 2\n",
    "for C in [0.1, 1, 10]:\n",
    "    for gamma in ['scale', 'auto']:\n",
    "        svc_param_grid_extended.append({\n",
    "            'C': C,\n",
    "            'kernel': 'poly',\n",
    "            'gamma': gamma,\n",
    "            'degree': 2\n",
    "        })\n",
    "\n",
    "svc_param_grid = svc_param_grid_extended\n",
    "\n",
    "# Résultats\n",
    "results_grid = []\n",
    "total_combinations = len(n_shots_values) * len(svc_param_grid)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"GRIDSEARCH: OPTIMISATION CONJOINTE DE n_shots ET PARAMÈTRES SVC\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nParamètres SVC: {len(svc_param_grid)} combinaisons\")\n",
    "print(f\"Valeurs n_shots: {len(n_shots_values)} valeurs\")\n",
    "print(f\"Total d'itérations: {total_combinations}\\n\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  • n_states_total = 5000\")\n",
    "print(f\"  • mixed_proportion = 0.1\")\n",
    "print(f\"  • Kernels: linear, rbf, poly(degree=2)\")\n",
    "print(f\"  • Métrique: F1-Score (weighted)\")\n",
    "print(f\"  • Cross-validation: 5-fold\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "iteration = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for n_shots in n_shots_values:\n",
    "    print(f\"\\n[n_shots = {n_shots:5d}] Génération dataset et GridSearch SVC...\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Générer le dataset avec ce n_shots\n",
    "    df_temp, _, y_temp = build_purity_classification_dataset(\n",
    "        n_shots=n_shots,\n",
    "        n_states_total=5000,\n",
    "        mixed_proportion=0.1\n",
    "    )\n",
    "    \n",
    "    X_temp_data = df_temp[['X_mean', 'Y_mean', 'Z_mean']]\n",
    "    X_temp_scaled = StandardScaler().fit_transform(X_temp_data)\n",
    "    \n",
    "    # Tester chaque combinaison de paramètres SVC\n",
    "    best_f1_for_n_shots = 0\n",
    "    best_config_for_n_shots = None\n",
    "    \n",
    "    for params in svc_param_grid:\n",
    "        iteration += 1\n",
    "        \n",
    "        # Créer et évaluer le modèle\n",
    "        svc_model = SVC(**params)\n",
    "        \n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        cv_scores = cross_val_score(\n",
    "            svc_model, X_temp_scaled, y_temp, \n",
    "            cv=5, scoring='f1_weighted'\n",
    "        )\n",
    "        \n",
    "        # Enregistrer les résultats\n",
    "        result = {\n",
    "            'n_shots': n_shots,\n",
    "            'C': params['C'],\n",
    "            'kernel': params['kernel'],\n",
    "            'gamma': params['gamma'],\n",
    "            'degree': params.get('degree', None),\n",
    "            'mean_f1': cv_scores.mean(),\n",
    "            'std_f1': cv_scores.std()\n",
    "        }\n",
    "        results_grid.append(result)\n",
    "        \n",
    "        # Afficher chaque exécution\n",
    "        degree_str = f\", degree={params.get('degree', 'N/A')}\" if params['kernel'] == 'poly' else \"\"\n",
    "        print(f\"  n_shots={n_shots:5d} | C={params['C']:4.1f} | kernel={params['kernel']:6s} | gamma={params['gamma']:6s}{degree_str} | F1={cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "        \n",
    "        # Tracker le meilleur pour ce n_shots\n",
    "        if cv_scores.mean() > best_f1_for_n_shots:\n",
    "            best_f1_for_n_shots = cv_scores.mean()\n",
    "            best_config_for_n_shots = result\n",
    "    \n",
    "    # Afficher le meilleur pour ce n_shots\n",
    "    print(f\"  → MEILLEUR POUR n_shots={n_shots}: F1={best_f1_for_n_shots:.4f} (C={best_config_for_n_shots['C']}, kernel={best_config_for_n_shots['kernel']}, gamma={best_config_for_n_shots['gamma']})\")\n",
    "\n",
    "# Créer un DataFrame avec tous les résultats\n",
    "results_grid_df = pd.DataFrame(results_grid)\n",
    "\n",
    "# Trouver le meilleur résultat global\n",
    "best_idx = results_grid_df['mean_f1'].idxmax()\n",
    "best_result = results_grid_df.loc[best_idx]\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"RÉSULTATS DU GRIDSEARCH - MEILLEURE CONFIGURATION GLOBALE\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\n✓ MEILLEUR n_shots: {int(best_result['n_shots'])}\")\n",
    "print(f\"\\n✓ MEILLEURS PARAMÈTRES SVC:\")\n",
    "print(f\"  • C: {best_result['C']}\")\n",
    "print(f\"  • kernel: {best_result['kernel']}\")\n",
    "print(f\"  • gamma: {best_result['gamma']}\")\n",
    "if best_result['degree'] is not None and best_result['kernel'] == 'poly':\n",
    "    print(f\"  • degree: {int(best_result['degree'])}\")\n",
    "print(f\"\\n✓ F1-Score (weighted): {best_result['mean_f1']:.4f} ± {best_result['std_f1']:.4f}\")\n",
    "print(f\"\\n✓ Temps d'exécution total: {elapsed_time:.2f} secondes\")\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e912534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "import time\n",
    "\n",
    "# Paramètres à optimiser\n",
    "n_shots_values = [10, 50, 100, 500, 1000, 5000]\n",
    "svc_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],  # Pas de 'poly' par défaut\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'degree': [2]  # Paramètre de degré pour polynomial\n",
    "}\n",
    "\n",
    "# Créer la grille des paramètres\n",
    "svc_param_grid = list(ParameterGrid(svc_params))\n",
    "\n",
    "# Ajouter manuellement le kernel polynomial de degré 2\n",
    "svc_param_grid_extended = []\n",
    "for params in svc_param_grid:\n",
    "    svc_param_grid_extended.append(params)\n",
    "\n",
    "# Ajouter les combinaisons avec polynomial de degré 2\n",
    "for C in [0.1, 1, 10]:\n",
    "    for gamma in ['scale', 'auto']:\n",
    "        svc_param_grid_extended.append({\n",
    "            'C': C,\n",
    "            'kernel': 'poly',\n",
    "            'gamma': gamma,\n",
    "            'degree': 2\n",
    "        })\n",
    "\n",
    "svc_param_grid = svc_param_grid_extended\n",
    "\n",
    "# Résultats\n",
    "results_grid = []\n",
    "total_combinations = len(n_shots_values) * len(svc_param_grid)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"GRIDSEARCH: OPTIMISATION CONJOINTE DE n_shots ET PARAMÈTRES SVC\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nParamètres SVC: {len(svc_param_grid)} combinaisons\")\n",
    "print(f\"Valeurs n_shots: {len(n_shots_values)} valeurs\")\n",
    "print(f\"Total d'itérations: {total_combinations}\\n\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  • n_states_total = 5000\")\n",
    "print(f\"  • mixed_proportion = 0.3\")\n",
    "print(f\"  • Kernels: linear, rbf, poly(degree=2)\")\n",
    "print(f\"  • Métrique: F1-Score (weighted)\")\n",
    "print(f\"  • Cross-validation: 5-fold\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "iteration = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for n_shots in n_shots_values:\n",
    "    print(f\"\\n[n_shots = {n_shots:5d}] Génération dataset et GridSearch SVC...\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Générer le dataset avec ce n_shots\n",
    "    df_temp, _, y_temp = build_purity_classification_dataset(\n",
    "        n_shots=n_shots,\n",
    "        n_states_total=5000,\n",
    "        mixed_proportion=0.3\n",
    "    )\n",
    "    \n",
    "    X_temp_data = df_temp[['X_mean', 'Y_mean', 'Z_mean']]\n",
    "    X_temp_scaled = StandardScaler().fit_transform(X_temp_data)\n",
    "    \n",
    "    # Tester chaque combinaison de paramètres SVC\n",
    "    best_f1_for_n_shots = 0\n",
    "    best_config_for_n_shots = None\n",
    "    \n",
    "    for params in svc_param_grid:\n",
    "        iteration += 1\n",
    "        \n",
    "        # Créer et évaluer le modèle\n",
    "        svc_model = SVC(**params)\n",
    "        \n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        cv_scores = cross_val_score(\n",
    "            svc_model, X_temp_scaled, y_temp, \n",
    "            cv=5, scoring='f1_weighted'\n",
    "        )\n",
    "        \n",
    "        # Enregistrer les résultats\n",
    "        result = {\n",
    "            'n_shots': n_shots,\n",
    "            'C': params['C'],\n",
    "            'kernel': params['kernel'],\n",
    "            'gamma': params['gamma'],\n",
    "            'degree': params.get('degree', None),\n",
    "            'mean_f1': cv_scores.mean(),\n",
    "            'std_f1': cv_scores.std()\n",
    "        }\n",
    "        results_grid.append(result)\n",
    "        \n",
    "        # Afficher chaque exécution\n",
    "        degree_str = f\", degree={params.get('degree', 'N/A')}\" if params['kernel'] == 'poly' else \"\"\n",
    "        print(f\"  n_shots={n_shots:5d} | C={params['C']:4.1f} | kernel={params['kernel']:6s} | gamma={params['gamma']:6s}{degree_str} | F1={cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "        \n",
    "        # Tracker le meilleur pour ce n_shots\n",
    "        if cv_scores.mean() > best_f1_for_n_shots:\n",
    "            best_f1_for_n_shots = cv_scores.mean()\n",
    "            best_config_for_n_shots = result\n",
    "    \n",
    "    # Afficher le meilleur pour ce n_shots\n",
    "    print(f\"  → MEILLEUR POUR n_shots={n_shots}: F1={best_f1_for_n_shots:.4f} (C={best_config_for_n_shots['C']}, kernel={best_config_for_n_shots['kernel']}, gamma={best_config_for_n_shots['gamma']})\")\n",
    "\n",
    "# Créer un DataFrame avec tous les résultats\n",
    "results_grid_df = pd.DataFrame(results_grid)\n",
    "\n",
    "# Trouver le meilleur résultat global\n",
    "best_idx = results_grid_df['mean_f1'].idxmax()\n",
    "best_result = results_grid_df.loc[best_idx]\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"RÉSULTATS DU GRIDSEARCH - MEILLEURE CONFIGURATION GLOBALE\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\n✓ MEILLEUR n_shots: {int(best_result['n_shots'])}\")\n",
    "print(f\"\\n✓ MEILLEURS PARAMÈTRES SVC:\")\n",
    "print(f\"  • C: {best_result['C']}\")\n",
    "print(f\"  • kernel: {best_result['kernel']}\")\n",
    "print(f\"  • gamma: {best_result['gamma']}\")\n",
    "if best_result['degree'] is not None and best_result['kernel'] == 'poly':\n",
    "    print(f\"  • degree: {int(best_result['degree'])}\")\n",
    "print(f\"\\n✓ F1-Score (weighted): {best_result['mean_f1']:.4f} ± {best_result['std_f1']:.4f}\")\n",
    "print(f\"\\n✓ Temps d'exécution total: {elapsed_time:.2f} secondes\")\n",
    "print(\"=\"*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
